---
title: 'Problem Set #4'
author: 'Mike Gruzynski'
output:
  html_document: default
---

<!--
Some guidelines for submitting problem sets in this course:

- Please submit a PDF document rather than a Word document or a Google document.
- Please put your name at the top of your problem set.
- Please **bold** or *highlight* your numerical answers to make them easier to find.
- If you'll be using `R` or `Python` code to calculate your answers, please put the code and its output directly into your Problem Set PDF document.
- It is highly recommended, although not required, that you use the RMarkdown feature in RStudio to compose your problem set answers. RMarkdown allows you to easily intermingle analysis code and answers in one document. It is of a similar design as `jupyter` and an ipython notebook.
- You do not need to show work for trivial calculations, but showing work is always allowed.
- For answers that involve a narrative response, please feel free to describe the key concept directly and briefly, if you can do so, and do not feel pressure to go on at length.
- Please ask us questions about the problem set if you get stuck. **Donâ€™t spend more than 20 minutes puzzling over what a problem means.** 
- Please ensure that someone (us!) can compile your solution set. The best way is to use the web-hosted links we've provided. 
-->

```{r}
# load packages 
library(foreign)
options(warn = 0)
```

# FE exercise 5.2
a. Make up a hypothetical schedule of potential outcomes for three Compliers and three Never-Takers where the ATE is positive but the CACE is negative. 

```{r}
# example dataframe with made up data to similate 3 Complier and 3 Never-takers for both treatment and control
df_5.2 = data.frame("type" = c('complier', 'complier', 'complier', 'never-taker', 'never-taker', 'never-taker'),
                    "Yi-d_0" = c(4, 1, 5, 2, 2, 5),
                    "Yi-d_1" = c(0, 3, 2, 8, 10, 9),
                    "di-z0" = c(0, 0, 0, 0, 0, 0),
                    "di-z1" = c(1, 1, 1, 0, 0, 0))

# using eq. 2.8 from FE
ATE_5.2a = mean(df_5.2$Yi.d_1) - mean(df_5.2$Yi.d_0)

# using eq 5.10 from FE
CACE_5.2a = mean(df_5.2[df_5.2$di.z1 == 1,]$Yi.d_1) - mean(df_5.2[df_5.2$di.z1 == 1,]$Yi.d_0)

cat("The hypothetical schedule yields an ATE of:", ATE_5.2a, "and a CACE of:", CACE_5.2a)
```

b. Suppose that an experiment were conducted on your pool of subjects. In what ways would the estimated CACE be informative or misleading? 

Since CACE is equal to ITT/ITT_D, if the ITT_D (or compliance rate) is very small then the calculated CACE can be overestimated in the estimated treatment. The amount of of subjects in the experiment might be unique and produce an intent to treat that is artificially large due to the lack of sample size and law of large numbers not valid to smooth values out. This can be fixed by adjusing them experimental design and mechanism in order to get a better compliance rate and sample size to estimate population values. In addition, it is often hard to figure out the never-takers and always takers in some experiments which will lead to improper estimates of effect calculations.

The CACE can be informative if the experimental set up is good and you want to see the causal effects for personality types ofcompliers (in order to help make an apples-to-apples comparison.)

c. **In addition, please also answer this question**: Which population is more relevant to study for future decision making: the set of Compliers, or the set of Compliers plus Never-Takers? Why?

There is no absolute answer for this question. I think it depends on what is the research question and experimental design. I think both groups could be of interest because Never-takers count could dictate if there is good randomization in the experiment when a placebo is added in. If the percentage of Never-takers is the same for both placebo never-takers and treatment never-takers than there is reason to assume that there is good randomization In addition not considering never-takers could take hurt evaluation of experiments and the two groups of compliers and never takers could answer why certain people dont comply and in addition could also create statistics to measure apples to apples comparisons (compliers vs compliers).


# FE exercise 5.6
Suppose that a researcher hires a group of canvassers to contact a set of 1,000 voters randomly assigned to a treatment group. When the canvassing effort concludes, the canvassers report that they successfully contacted 500 voters in the treatment group, but the truth is that they only contacted 250. When voter turnout rates are tabulated for the treatment and control groups, it turns out that 400 of the 1,000 subjects in the treatment group voted, as compared to 700 of the 2,000 subjects in the control group (none of whom were contacted). 

```{r}
# information needed to answer question 5.6
treatment_total_5.6 = 1000
report_contacted_total_5.6 = 500
actually_contacted_total_5.6 = 250

report_treatment_group_voted_5.6 = 400
report_control_group_voted_5.6 = 700
control_total_5.6 = 2000
```

a. If you believed that 500 subjects were actually contacted, what would your estimate of the CACE be? 

```{r}
ITT_5.6a = (report_treatment_group_voted_5.6/treatment_total_5.6) - 
  (report_control_group_voted_5.6/control_total_5.6)
compliance_rate_5.6a = report_contacted_total_5.6/treatment_total_5.6

CACE_5.6a = ITT_5.6a/compliance_rate_5.6a
cat("The CACE for the situation in question 5.6a is equal to:", CACE_5.6a)
```

b. Suppose you learned that only 250 subjects were actually treated. What would your estimate of the CACE be? 

```{r}
ITT_5.6b = (report_treatment_group_voted_5.6/treatment_total_5.6) - 
  (report_control_group_voted_5.6/control_total_5.6)
compliance_rate_5.6b = actually_contacted_total_5.6/treatment_total_5.6

CACE_5.6b = ITT_5.6b/compliance_rate_5.6b
cat("The CACE for the situation in question 5.6a is equal to:", CACE_5.6b)
```

c. Do the canvassers' exaggerated reports make their efforts seem more or less effective? Define effectiveness either in terms of the ITT or CACE. Why does the definition matter? 

The exaggerated reports from the canvassers actual underestimate the effectiveness of the study. Since CACE = ITT / compliance_rate, the exageration makes the compliance_rate equal to 0.5 rather than the actual compliance_rate equal to 0.25. Since compliance_rate is on the denominator the CACE will reduce from ATE \* 4 (actual) to ATE \* 2 (exaggerated reports) that will create a reduction of CACE by 0.5 the actual reported CACE. The ITT does not change based on any of the canvassers' exageration, only CACE (in this situation).

# FE exercise 5.10
Guan and Green report the results of a canvassing experiment conduced in Beijing on the eve of a local election. Students on the campus of Peking University were randomly assigned to treatment or control groups. Canvassers attempted to contact students in their dorm rooms and encourage them to vote. No contact with the control group was attempted. Of the 2,688 students assigned to the treatment group, 2,380 were contacted. A total of 2,152 students in the treatment group voted; of the 1,334 students assigned to the control group, 892 voted. One aspect of this experiment threatens to violate the exclusion restriction. At every dorm room they visited, even those where no one answered, canvassers left a leaflet encouraging students to vote. 

```{r}
library(foreign)
d_5.10 <- read.dta("./data/Guan_Green_CPS_2006.dta")
head(d_5.10)

total_5.10 = 2688
contacted_5.10 = 2380
treatment_voted_5.10 = 2152
control_5.10 = 1334
control_voted_5.10 = 892
```

a. Using the data set from the book's website, estimate the ITT. First, estimate the ITT using the difference in two-group means. Then, estimate the ITT using a linear regression on the appropriate subset of data. *Heads up: There are two NAs in the data frame. Just na.omit to remove these rows.*

```{r}
library(lmtest)
library(sandwich)

df_treat_5.10a = na.omit(d_5.10[d_5.10$treat2 == 1,])
df_control_5.10a = na.omit(d_5.10[d_5.10$treat2 == 0,])

ITT_eq_5.10a = mean(df_treat_5.10a$turnout) - mean(df_control_5.10a$turnout)
ITT_fit_5.10a = lm(turnout ~ treat2, data=d_5.10)
ITT_fit_5.10a_value = summary(ITT_fit_5.10a)$coefficients[2]

cat("The ITT for the situation in question 5.10 calculated by euation is equal to:", ITT_eq_5.10a, "and the \n ITT calculated by OLS regression is equal to:", ITT_fit_5.10a_value)
```

b. Use randomization inference to test the sharp null hypothesis that the ITT is zero for all observations, taking into account the fact that random assignment was clustered by dorm room. Interpret your results. 

```{r}
itt_list_5.10a = c()
for (i in 1:500){
  d_random = sample(d_5.10$treat2)
  d_5.10$random_inference = d_random
  ITT_ri_fit_5.10b = lm(turnout ~ random_inference + as.factor(dormid), data=d_5.10)
  itt_list_5.10a = c(itt_list_5.10a, summary(ITT_ri_fit_5.10b)$coefficients[2])
}
p_value_5.10a = mean(itt_list_5.10a >= ITT_fit_5.10a_value)
plot(density(itt_list_5.10a), main = "RI Density Plot: 5.10b")
cat("P-value of ITT from 5.10a < the list of ITT when controlled by dormid is equal to:", p_value_5.10a)
```

Looking at the above data, we see that the ITT value obtained above has a p-value ~ 0  when controlled for dormid and have statistical evidence to reject the null hypothesis that ITT is zero for all observations. when taking into account the fact that clustering was done on dorms, the intent to treat had actual little effect (based on dorms that had large amount of compliers and some dorms had large amount of never-takers). This leads to a smearing out effect of the intent to treat and is shown above with the RI density plot.

c. Assume that the leaflet had no effect on turnout. Estimate the CACE. Do this in two ways: First, estimate the CACE using means. Second, use some form of linear model to estimate this as well. If you use a 2SLS, then report the standard errors and draw interence about whether the leaflet had any causal effect among compliers. 

```{r}
library(AER)

ITT_eq_5.10c = mean(df_treat_5.10a$turnout) - mean(df_control_5.10a$turnout)
ITTD_eq_5.10c = mean(df_treat_5.10a$contact)
cace_eq_5.10c = ITT_eq_5.10c/ITTD_eq_5.10c

# source: https://github.com/UCB-MIDS/experiments-causality/blob/master/code/week8codeANSWERS.R
cace_fit_5.10c <- ivreg(turnout ~ contact, ~treat2, data = d_5.10)
cace_fit_5.10c_value = summary(cace_fit_5.10c)$coefficients[2]
coeftest(cace_fit_5.10c, vcovHC(cace_fit_5.10c))

cat("The CACE for the situation in question 5.10c calculated by equation is equal to:", cace_eq_5.10c, "and the CACE calculated by OLS regression is equal to:", cace_fit_5.10c_value)

# DID NOT USE 2SLS so above statement: If you use a 2SLS, then report the standard errors
# Does not apply
```

There is not much of a causal effect from the leaflet amoung the compliers. The intent to treat effect is ~ 0.132 (13.2% turnout difference due to treatment) and the CACE was ~ 0.149 (14.9% turnout difference due to treatment with compliance to the treatment). There is not much difference in the values between the turnout rates (~ 1.7%) and the fact that the canvassers left the leaflet at the dorm door means that anyone walking in the dorm by the door could have seen the flyer. This could lead to spillage over to the control group and actually lead to an underestimation of the effect from the treatment - shrinking effect.

d. *SKIP*
e. *SKIP*
f. *SKIP* 

# FE exercise 5.11
Nickerson describes a voter mobilization experiment in which subjects were randomly assigned to one of three conditions: a baseline group (no contact was attempted); a treatment group (canvassers attempted to deliver an encouragement to vote); and a placebo group (canvassers attempted to deliver an encouragement to recycle). Based on the results in the table below answer the following questions 

+----------------------+-----------+------+---------+
| Treatment Assignment | Treated ? | N    | Turnout |
+======================+===========+======+=========+
| Baseline              | No       | 2572 | 31.22%  |
+----------------------+-----------+------+---------+
| Treatment            | Yes       | 486  | 39.09%  |
+----------------------+-----------+------+---------+
| Treatment            | No        | 2086 | 32.74%  |
+----------------------+-----------+------+---------+
| Placebo              | Yes       | 470  | 29.79%  |
+----------------------+-----------+------+---------+
| Placebo              | No        | 2109 | 32.15%  |
+----------------------+-----------+------+---------+

**First** Use the information to make a table that has a full recovery of this data. That is, make a `data.frame` or a `data.table` that will have as many rows a there are observations in this data, and that would fully reproduce the table above. (*Yes, this might seem a little trivial, but this is the sort of "data thinking" that we think is important.*)

```{r}
baseline_noT_turnout = round(2572 * 0.3122)
baseline_noT_no_turnout = 2572 - baseline_noT_turnout
treatment_yesT_turnout = round(486 * 0.3909)
treatment_yesT_no_turnout = 486 - treatment_yesT_turnout
treatment_noT_turnout = round(2086 * 0.3274)
treatment_noT_no_turnout = 2086 - treatment_noT_turnout
placebo_yesT_turnout = round(470 * .2979)
placebo_yesT_no_turnout = 470 - placebo_yesT_turnout
placebo_noT_turnout = round(2109 * .3215)
placebo_noT_no_turnout = 2109 - placebo_noT_turnout

df_5.11 = data.frame(treatment = c(rep(0, 2572), rep(1, (486 + 2086)), rep(0, (470 + 2109))),
                     placebo = c(rep(0, (2572 + 486 + 2086)), rep(1, (470 + 2109))),
                     complied = c(rep(0, 2572), rep(1, 486), rep(0, 2086), rep(1, 470), rep(0, 2109)),
                     turnout = c(rep(1, baseline_noT_turnout), rep(0, baseline_noT_no_turnout),
                                 rep(1, treatment_yesT_turnout), rep(0, treatment_yesT_no_turnout),
                                 rep(1, treatment_noT_turnout), rep(0, treatment_noT_no_turnout),
                                 rep(1, placebo_yesT_turnout), rep(0, placebo_yesT_no_turnout),
                                 rep(1, placebo_noT_turnout), rep(0, placebo_noT_no_turnout))
                     )

```



a. We are rewriting part (a) as follows: "Estimate the proportion of Compliers by using the data on the Treatment group.  Then compute a second estimate of the proportion of Compliers by using the data on the Placebo group.  Are these sample proportions statistically significantly different from each other?  Explain why you would not expect them to be different, given the experimental design." (Hint: ITT_D means "the average effect of the treatment on the dosage of the treatment." I.E., it's the contact rate $\alpha$ in the async).

```{r}
treatment_compliers_group = mean(df_5.11[df_5.11$treatment == 1,]$complied)
placebo_compliers_group = mean(df_5.11[df_5.11$placebo == 1,]$complied)

cat("The proportion of compliers for treatment group is equal to:", treatment_compliers_group, "and the proportion of compliers for placebo group is equal to:", placebo_compliers_group)

t.test(df_5.11[df_5.11$treatment == 1,]$complied, df_5.11[df_5.11$placebo == 1,]$complied, paired = F)
```

The above t-test shows that the difference in means 95% confidence interval range is eaula to: [-0.01452611  0.02795977], and shows that the range of values cross over the value of zero. The p-value of the test is equal to ~ 0.54 meaning that we fail to reject the null hypothesis that the difference of means is equal to zero. 

This difference of means is not surprising. Since the groups are created inside of an experiment, I am making an assumption that there were experiment design mechanisms inside of the experiment (in other words randomly selection of treatment and placebo). The proportion of contacted subjects should be equivalent because a lot of time the majority of people do not want to talk to anyone who comes to their door with anything (whether it is about a political campaign information or information about something not emotionally threatening like free ice cream). The amount contacted in control and placebo should be similar in compliance rate as long as they experimental mechanism for each group information transfer is a similar activity.

b. Do the data suggest that Never Takers in the treatment and placebo groups have the same rate of turnout? Is this comparison informative? 

```{r}
t.test(df_5.11[df_5.11$treatment == 1 & df_5.11$complied == 0,]$turnout, df_5.11[df_5.11$placebo == 1 & df_5.11$complied == 0,]$turnout, paired = F)
```

The above test shows a difference in means t.test shows that there is strong evidence (p-value ~ 0.68) to fail to reject the null hypothesis that the difference in means (turnout rate in experiment groups) is equal to zero. This suggest that the never takers in the treatment and control groups have very similar rate of turnout. In order to improve the informative information on the comparison, I think you need to include a difference in means test in between placebo never takers and baseline and also treatment never takers and baseline. If all of these three comparisons (above t.test and below 2 t.test) show no evidence to reject the null hypothesis then we can say with confidence that the attempt to treat (non-compliance in treatment or placebo) did not alter the turnout rate of the election and the experiment was set up properly in order to create an environment to compare apples to apples (experimental groups didnt have a bias inside of them that intrinsically pulled the group turnout rate up or down).

```{r}
t.test(df_5.11[df_5.11$treatment == 1 & df_5.11$complied == 0,]$turnout, df_5.11[df_5.11$treatment == 0,]$turnout, paired = F)
t.test(df_5.11[df_5.11$placebo == 1 & df_5.11$complied == 0,]$turnout, df_5.11[df_5.11$treatment == 0,]$turnout, paired = F)
```

The above t.test for difference in means shows never takers in treatment and baseline (p-value ~ 0.29) and never takers in placebo and baseline (p-value ~ 0.5737) both fail to reject the null hypothesis that the difference in means is equal to zero.

c. Estimate the CACE of receiving the placebo. Is this estimate consistent with the substantive assumption that the placebo has no effect on turnout? 

```{r}
ITT_5.11c = mean(df_5.11[df_5.11$placebo == 1,]$turnout) - mean(df_5.11[df_5.11$treatment == 0 & df_5.11$placebo == 0,]$turnout)
itt_d_5.11c = mean(df_5.11[df_5.11$placebo == 1,]$complied)

CACE_5.11c = ITT_5.11c / itt_d_5.11c
CACE_5.11c
```

Yes, the estimate of the CACE of receiving placebo shows that there is a very low treatment effect for being in placebo and being in treatment (controlling for compliance in placebo). The value of the CACE is around 0.027 (2.7%), which is very practically insignificant in of the effect in being in the placebo group.

d. Estimate the CACE of receiving the treatment using two different methods. First, use the conventional method of dividing the ITT by the ITT_{D}. 

```{r}
ITT_5.11d = mean(df_5.11[df_5.11$treatment == 1,]$turnout) - mean(df_5.11[df_5.11$treatment == 0,]$turnout)
itt_d_5.11d = mean(df_5.11[df_5.11$treatment == 1,]$complied)

CACE_5.11d = ITT_5.11d / itt_d_5.11d
cat("The CACE of receiving the treatment for question 5.10d is:", CACE_5.11d)
```

e. Then, second, compare the turnout rates among the Compliers in both the treatment and placebo groups. Interpret the results. 

```{r}
CACE_5.11e = mean(df_5.11[df_5.11$treatment == 1 & df_5.11$complied == 1,]$turnout) - mean(df_5.11[df_5.11$placebo == 1 & df_5.11$complied == 1,]$turnout)

cat("The CACE of receiving the treatment for question 5.10e is:", CACE_5.11e)
```

The CACE calculated by comparing the turnout rates among the compliers in both the treatment and placebo groups is different from the CACE calculated from dividing the ITT by the ITT_{D}. The one calculated from the equation (5.10d) is estimated from a group of compliers and never takers. In the placebo comparison version (5.10e), contains two compared groups that are full of "full" compliers. The comparison CACE (placebo design) is to create an experimental mechanism that isolates random sample of compliers in both groups for more clear treatment effects. It helps compare subjects who are willing to comply with treatment information to subjects who are willing to comply to a placebo treatment.According to Gerber and Green Field Experiments (page 162-163), the comparing method with the placebo is preferible over the equation version when ITT_{D} is less than 0.5 and that a weakness to the placebo design is when number of treatment is hard to obtain (since placebo splits off possible treatment and control subjects into a placebo group) and that there is a danger of bias (but can be mitigated with proper experimental design). The placbo mechanism also leads to stronger statistical power, which means that you are less likely to accept the null hypothesis when it is false (less likely to make a Type II error).

<!--
# EVERYTHING IN THIS COMMENTED SECTION IS NOT REQUIRED. THESE ARE GOOD PROBLEMS, AND IF YOU WANT TO CHECK YOUR 
# UNDERSTANDING, THEY WOULD BE GOOD TO DO. 

# More Practice 
Determine the direction of bias in estimating the ATE for each of the following situations when we randomize at the individual level.  Do we over-estimate, or underestimate? Briefly but clearly explain your reasoning.

a. In the advertising example of Lewis and Reiley (2014), assume some treatment-group members are friends with control-group members.

b. Consider the police displacement example from the bulleted list in the introduction to FE 8, where we are estimating the effects of enforcement on crime.

c. Suppose employees work harder when you experimentally give them compensation that is more generous than they expected, that people feel resentful (and therefore work less hard) when they learn that their compensation is less than others, and that some treatment-group members talk to control group members.

d. When Olken (2007) randomly audits local Indonesian governments for evidence of corruption, suppose control-group governments learn that treatment-group governments are being randomly audited and assume they are likely to get audited too.


# FE exercise 8.2
National surveys indicate that college roommates tend to have correlated weight. The more one roommate weights at the end of the freshman year, the more the other freshman roommate weights. On the other hand, researchers studying housing arrangements in which roommates are randomly paired together find no correlation between two roommates' weights at the end of their freshman year. *Explain how these two facts can be reconciled.*
-->


# FE exercise 8.10
A doctoral student conducted an experiment in which she randomly varied whether she ran or walked 40 minutes each morning. In the middle of the afternoon over a period of 26 days she measured the following outcome variables: (1) her weight; (2) her score in Tetris; (3) her mood on a 0-5 scale; (4) her energy; and (5) whether she got a question right on the math GRE. 

```{r}
library(foreign)
d_8.10 <- read.dta("./data/Hough_WorkingPaper_2010.dta")
head(d_8.10)
``` 

a. Suppose you were seeking to estimate the average effect of running on her Tetris score. Explain the assumptions needed to identify this causal effect based on this within-subjects design. Are these assumptions plausible in this case? What special concerns arise due to the fact that the subject was conducting the study, undergoing the treatments, and measuring her own outcomes? 

The assumptions needed to successfully make within-subjects design causal claims is the no-anticipation assumption and the no-persistence asumption. The no-anticipation assumption states thats that the potential outcomes are unaffected by treatments that are administrated in the future. The no persistence assumption states that potential outcomes in one period are unaffected by treatments administrated in prior periods. The assumptions could be plausable, however it depends on the subject personality. In reality, it seems not realistic to assume these asumptions. The anticipation assumption seems non plausable because if the runner does not like running and the day that is a run day, the subject can be primed to having a bad day and move throughout the day less happy which can effect the mood of the subject which can also effect the tetris score. In addition to anticipation assumption, the persistence assumption can be hard to prove considering that if the running is done multiple days in a row that the subject can get soar or even injured due to multiple run events with no rest days ("wash-out" periods) or walking days.

There are a couple of things that are wrong with the fact that the subject is the one conducting the experiment. The run binary selection variable could not be truly random, especially if the subject is sick of running or too soar to run and switch to walking even though it was suppose to be a run day. In addition, there is no information about how the mood and energy was scored. If tere is no objective test the subject takes everyday, then the subject could be giving an objective response with no logic to the answer. Also, there is no mechanism in the experiment that makes the subject accurately follow the experiment's randomization and record data honestly. Also, when does the recording of data or tetris playing happen (morning, afternoon, before the run/walk or after)? This could effect the reporting and running of the experiment.

b. Estimate the effect of running on Tetris score. Use regression to test the sharp null hypothesis that running has no immediate or lagged effect on Tetris scores. (**Note** the book calls for randomization inference, but this is a bit of a tough coding problem. **HINT**: For the second part of part (b), run one regression of today's score on both today's treatment assignment and yesterday's treatment assignment. Then, calculate the p-value that both effects are zero.)

```{r}
library(forecast)
library(stats)

lm_8.2_b = lm(tetris ~ run, data=d_8.10)
cat("The effect of running on Tetris score is equal to:", lm_8.2_b$coefficients[[2]])
```

```{r}
d_8.10['lagged_run'] = c(NA, d_8.10$run[1:length(d_8.10$run) - 1])
lm_8.2_b_lagged =  lm(tetris ~ run + lagged_run, data=d_8.10)
summary(lm_8.2_b_lagged)
```

The above OLS equation with run and lag run shows that the run term is statistically significant and has some sort of effect on tetris scores (p-value ~ 0.006) with statistical confidence interval of 95% shown below (It does not cross zero). The lagged run value does no show statistical significance at any classic threshold and has a 95% confidence interval on both sides of zero (shown below):

```{r}
confint(lm_8.2_b_lagged)
```

```{r}
ri_list_8.2b_run_coef = c()
ri_list_8.2b_run_lag_coef= c()
for (i in 1:10000){
  d_8.10['run_random_inference'] = sample(d_8.10$run)
  d_8.10['lagged_run_random_inference'] = c(NA, d_8.10$run_random_inference[1:length(d_8.10$run_random_inference) - 1])
  lm_filtered_8.2b =  lm(tetris ~ run_random_inference + lagged_run_random_inference, data=d_8.10)
  ri_list_8.2b_run_coef = c(ri_list_8.2b_run_coef, summary(lm_filtered_8.2b)$coefficients[2][[1]])
  ri_list_8.2b_run_lag_coef = c(ri_list_8.2b_run_lag_coef, summary(lm_filtered_8.2b)$coefficients[3][[1]])
}

par(mfrow=c(1,2))
plot(density(ri_list_8.2b_run_coef), main = "RI Density Plot: RUN COEF")
plot(density(ri_list_8.2b_run_lag_coef), main = "RI Density Plot: RUN_LAG COEF")
```

However, with running a RI on the OLS equation based on run and run lag, it appears that the sharp null is valid of coefficients = 0. This is explained in part e below, but the short version is that a majority of the large values are later in the running schedule (probably due to getting better at the game), so that if a run day hapened to be later in the schedule it was likly to have a larger value of score.

```{r}
average_run_days_first = round(mean(d_8.10$run[1:13]), 3) * 100
average_run_days_second = round(mean(d_8.10$run[14:26]), 3) * 100

cat("The above mean calculations show that in the second half of the trail there was:", average_run_days_first, "% vs.", average_run_days_second, "% in the second half of the experiment. The majority of the larger values for tetris scores are in the later half of the trial -- probably skill at game improving and skewing the effects of the run day due to uneven counts of treatment vs baseline in regions of the experiment.")
```

c. One way to lend credibility to with-subjects results is to verify the no-anticipation assumption. Use the variable `run` to predict the `tetris` score *on the preeceding day*. Presume that the randomization is fixed. Why is this a test of the no-anticipation assumption? Does a test for no-anticipation confirm this assumption? 

```{r}
d_8.10c = d_8.10
d_8.10c_tetris_list = c(NA, d_8.10$tetris[1:length(d_8.10$tetris) - 1])
d_8.10c['preceding_score'] = d_8.10c_tetris_list

lm_8.2_c = lm(preceding_score ~ run, data=d_8.10c)
summary(lm_8.2_c)
```

The no-anticipation assumption states thats that the potential outcomes are unaffected by treatments that are administrated in the future. So, if we try to predict tetris score from the previous day (i-1) to the dummy variable of run (i) then we should not see a statistical significant coeficient value for the run variable in the regression. Above, is the summary table and the coefficient for run is estimated at ~ 645.6 with a standard error of 4823.5. This means the 95% confidence iterval is :

```{r}
confint(lm_8.2_c)
```

The range of the above confidence interval shows that there is not predictive power of run on preceding tetris score, showing that there contains reason to believe that the no-anticipation assumption is valid.

d. If Tetris responds to exercise, one might suppose that energy levels and GRE scores would as well. Are these hypotheses borne out by the data? 

```{r}
library(stats)
run0_gre = d_8.10[d_8.10$run == 0,]
run1_gre = d_8.10[d_8.10$run == 1,]

run0_avgGRE = nrow(run0_gre[run0_gre$gre == 1,])/nrow(run0_gre)
run1_avgGRE = nrow(run1_gre[run1_gre$gre == 1,])/nrow(run1_gre)

cat("On days where subject ran, the GRE dummy score was on average:", run1_avgGRE, "\n")
cat("On days where subject walked, the GRE dummy score was on average:", run0_avgGRE, "\n")

energy1_run = d_8.10[d_8.10$energy == 1,]
energy2_run = d_8.10[d_8.10$energy == 2,]
energy3_run = d_8.10[d_8.10$energy == 3,]
energy4_run = d_8.10[d_8.10$energy == 4,]
energy5_run = d_8.10[d_8.10$energy == 5,]

run_percentage_energy = c(nrow(energy1_run[energy1_run$run == 1,])/nrow(energy1_run),
  nrow(energy2_run[energy2_run$run == 1,])/nrow(energy2_run),
  nrow(energy3_run[energy3_run$run == 1,])/nrow(energy3_run),
  nrow(energy4_run[energy4_run$run == 1,])/nrow(energy4_run),
  nrow(energy5_run[energy5_run$run == 1,])/nrow(energy5_run))
count_percentage_energy = c(nrow(energy1_run[energy1_run$run == 1,]),
  nrow(energy2_run[energy2_run$run == 1,]),
  nrow(energy3_run[energy3_run$run == 1,]),
  nrow(energy4_run[energy4_run$run == 1,]),
  nrow(energy5_run[energy5_run$run == 1,]))
energy_levels = c(1, 2, 3, 4, 5)

library(latticeExtra)
obj1 <- xyplot(run_percentage_energy ~ energy_levels, type = "l")
obj2 <- xyplot(count_percentage_energy ~ energy_levels, type = "l")
doubleYScale(obj1, obj2, add.axis = TRUE, add.ylab2 = TRUE, text = c("Run Percentage", "Run Count"))
```

We can not say running causes the GRE score to be recorded as equal to 1 because on days the subject ran the average GRE dummy variable is lower than when subject didnt run, but this could also be due to the fact that the subject ran more than not ran.

The situation that running will improve energy is not a valid claim. From the above xyplot, we can see that there is no trend (energy level 3 dips far below energy levels 2 and 4) that shows that higher energy levels also had more running days as the treatment for that day. The energy levels for 1 and 2 had greater than ~0.75% of run days. In fact, there are more run days than non run days and that could be effecting the cause and effect. 

Also, we dont know when the actual days are (of the week), which could be a huge influence on energy. There are some ommitted variables in this analysis and becuase of that no claims of running on energy can be made or what else the subject was subjected to during the experimental day (stress from work/school/relationship etc.). So both data claims are not "borne out by the data"

e. **Additional Mandatory Question**: Note that the observations in this regression are not necessarily all independent of each other. In particular, think about what happens when we lag a variable. Given this non-independence, would you expect randomization inference to give you a better answer than the regression answer you just obtained in (b)? Why? Which number(s) do you expect to be different in regression than in randomization inference?  What is the direction of the bias?  (This is a conceptual question, so you do not need to conduct the randomization inference to answer it. However, you are certainly welcome to try that exercise if you are curious.)

No, I do not expect randomization inference to give you a better answer than the regression answer you just obtained in (b) with the lag variable. This is due to the fact that over time the subject is also getting better at the game. There is a time dependent omitted variable here of game skill, and in theory it should be getting better the more and more you play the game. Random inference will jumble the order and mess up the "skill" part of playing tetris. The prediction power of any of the variables will be different and should be all near no prediction power (zero) if there is a good share of the amount in the beginning of the trial and at the end of the trial (which should be correct if randomization of order is proper and enough samples was taken). The reason is because there are large numbers at the end of the experiment (which probably stems from skill - the subject played the game for a while now) and they will belong to different buckets due to randomization and should smear out. Because of that, I would expect that randomization inference underestimates the effects due to not taking into the account time dependent skill and a majority of larger values are in the later weeks, which would be all jumbled and smeared in random inference.
